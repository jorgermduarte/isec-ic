{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dir = \"..\\isec-ic\\\\dataset\\\\train\"\n",
    "validation_dir = \"..\\isec-ic\\\\dataset\\\\valid\"\n",
    "test_dir = \"..\\isec-ic\\\\dataset\\\\test\"\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath(__file__ if '__file__' in locals() else os.getcwd()))\n",
    "train_dir = os.path.join(current_dir, train_dir)\n",
    "validation_dir = os.path.join(current_dir, validation_dir)\n",
    "test_dir = os.path.join(current_dir, test_dir)\n",
    "\n",
    "categories = [\"bacterialspot\", \"healthy\",\"lateblight\",\"leafmold\", \"mosaicvirus\",\"yellowleafcurlvirus\" ,\"spidermite\",\"septorialeafspot\" ]\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    subfolders = os.listdir(folder)\n",
    "    for subfolder in subfolders:\n",
    "        category = subfolder.split(\"_\")[-1].lower()\n",
    "        if category in categories:\n",
    "            print(\"Loading images from category: \" + category + \" and subfolder: \" + subfolder)\n",
    "            subfolder_path = os.path.join(folder, subfolder)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                img = Image.open(os.path.join(subfolder_path, filename))\n",
    "                img = img.resize((64, 64))\n",
    "                img = np.array(img) / 255.0\n",
    "                images.append(img)\n",
    "                labels.append(categories.index(category))\n",
    "        else:\n",
    "            print(\"Category: \" + category + \" from subfolder \" + subfolder + \" is not in the list of categories.\")\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "x_train, y_train = load_images_from_folder(train_dir)\n",
    "x_validation, y_validation = load_images_from_folder(validation_dir)\n",
    "x_test, y_test = load_images_from_folder(test_dir)\n",
    "\n",
    "print(\"Images loaded successfully\")\n",
    "\n",
    "reduction_ratio = 0.8\n",
    "\n",
    "x_validation, x_reduction_validation, y_validation, y_reduction_validation = train_test_split(x_validation, y_validation, test_size=reduction_ratio, random_state=42)\n",
    "print(\"Conjunto de validação reduzido em 80%\")\n",
    "\n",
    "x_train, x_reduction, y_train, y_reduction = train_test_split(x_train, y_train, test_size=reduction_ratio, random_state=42)\n",
    "print(\"Conjunto de treino reduzido em 80%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
    "\n",
    "def create_cnn_model(dropout_rate, learning_rate):\n",
    "    # Garantir que o dropout_rate esteja no intervalo [0, 1)\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "    #base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(64, 64, 3)))\n",
    "    #base_model = Xception(weights='imagenet', include_top=False, input_tensor=Input(shape=(64, 64, 3)))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add the dense layers (top layers)\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)  # Flatten the output layer to 1 dimension\n",
    "    x = Dense(512, activation='relu')(x)   #  First dense layer\n",
    "    x = Dropout(dropout_rate)(x)  # Dropout to reduce overfitting\n",
    "    predictions = Dense(len(categories), activation='softmax')(x)  # Output layer\n",
    "\n",
    "    # Final model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"Create_cnn_model function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# Define a função para limitar a afinidade do processador\n",
    "def limitar_afinidade(processo, num_cores):\n",
    "    if num_cores < 1:\n",
    "        num_cores = 1\n",
    "    elif num_cores > psutil.cpu_count(logical=False):\n",
    "        num_cores = psutil.cpu_count(logical=False)\n",
    "\n",
    "    processo.cpu_affinity(list(range(num_cores)))\n",
    "\n",
    "# Obtém o ID do processo atual\n",
    "pid = os.getpid()\n",
    "\n",
    "# Define o número de núcleos desejado (metade dos núcleos disponíveis)\n",
    "metade_dos_cores = psutil.cpu_count(logical=False) // 2\n",
    "\n",
    "# Limita a afinidade do processador\n",
    "limitar_afinidade(psutil.Process(pid), metade_dos_cores)\n",
    "\n",
    "# Restaura a afinidade do processador para todos os núcleos após a execução do código\n",
    "limitar_afinidade(psutil.Process(pid), psutil.cpu_count(logical=False))\n",
    "\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU disponível. Usando GPU para treinamento.\")\n",
    "else:\n",
    "    print(\"GPU não disponível. Usando CPU para treinamento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarm.pso import pso\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print(\"Iniciando a otimização com PSO...\")\n",
    "dimension = 2    # Número de dimensões (neste caso, dropout_rate e learning_rate)\n",
    "lb = [0.1, 0.0001]   # Limites inferiores para dropout_rate e learning_rate\n",
    "ub = [1, 0.01]    # Limites superiores para dropout_rate e learning_rate\n",
    "\n",
    "num_agents = 2  # Número de agentes\n",
    "max_iter = 5    # Número de iterações\n",
    "k_n_splits = 3 # k splits \n",
    "\n",
    "def evaluate_model_PSO(solution):\n",
    "    print(f\" - Evaluating with cross validation, splits={k_n_splits}\")\n",
    "    dropout_rate, learning_rate = solution\n",
    "    dropout_rate = abs(dropout_rate)\n",
    "    learning_rate = abs(learning_rate)\n",
    "    kfold = KFold(n_splits=k_n_splits, shuffle=True, random_state=42) \n",
    "    auc_scores = []\n",
    "\n",
    "    for train, val in kfold.split(x_train, y_train):\n",
    "        model = create_cnn_model(dropout_rate, learning_rate)\n",
    "        \n",
    "        # Treina o modelo com os dados de treino e valida nos dados de validação\n",
    "        model.fit(x_train[train], y_train[train], epochs=5, batch_size=32, verbose=1)\n",
    "        y_pred = model.predict(x_train[val])\n",
    "        y_val_categorical = to_categorical(y_train[val], num_classes=len(categories))\n",
    "        auc_score = roc_auc_score(y_val_categorical, y_pred, multi_class='ovr')\n",
    "        auc_scores.append(auc_score)\n",
    "        print(f\"   -- generating model with -> dropout_rate: {dropout_rate}, learning_rate: {learning_rate}, auc score: {auc_score}\")\n",
    "\n",
    "    auc_median_score = 1 - np.mean(auc_scores)   # Minimizar 1 - AUC\n",
    "    print(f\" - generated cnn models -> dropout_rate: {dropout_rate}, learning_rate: {learning_rate}, auc median score (minimized): {auc_median_score}\")\n",
    "    \n",
    "    # Retorna a média do AUC score\n",
    "    return auc_median_score\n",
    "\n",
    "\n",
    "# Criando a instância do PSO\n",
    "pso_instance = pso(n=num_agents, function=evaluate_model_PSO, lb=lb, ub=ub, dimension=dimension, iteration=max_iter)\n",
    "best_solution = pso_instance.get_Gbest()\n",
    "\n",
    "best_dropout_rate, best_learning_rate = best_solution\n",
    "best_dropout_rate = abs(best_dropout_rate)\n",
    "best_learning_rate = abs(best_learning_rate)\n",
    "print(f\"Melhores parâmetros encontrados: \\nBest dropout rate: {best_dropout_rate}, Best Learning Rate: {best_learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#best_learning_rate = 0.003889651856290339 \n",
    "#best_dropout_rate = 0.7057278672251002\n",
    "\n",
    "x_train = x_reduction\n",
    "x_validation = x_reduction_validation\n",
    "\n",
    "y_train = y_reduction\n",
    "y_validation =  y_reduction_validation\n",
    "\n",
    "print(\"Treinando o modelo com os melhores hiperparâmetros...\")\n",
    "best_model = create_cnn_model(best_dropout_rate, best_learning_rate)\n",
    "best_model_history = best_model.fit(x_train, y_train, epochs=20, batch_size=32, verbose=1, validation_data=(x_validation, y_validation))\n",
    "\n",
    "best_pso_train_accuracy = best_model_history.history['accuracy'][-1]\n",
    "best_pso_validation_accuracy = best_model_history.history['val_accuracy'][-1]\n",
    "\n",
    "print(\"Melhor Accuracy de Treinamento:\", best_pso_train_accuracy)\n",
    "print(\"Melhor Accuracy de Validação:\", best_pso_validation_accuracy)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotar as curvas de treino e validação\n",
    "plt.plot(best_model_history.history['accuracy'], label='Treinamento')\n",
    "plt.plot(best_model_history.history['val_accuracy'], label='Validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avaliando o modelo no conjunto de teste...\")\n",
    "loss_test, test_accuracy = best_model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Acurácia no Teste: {test_accuracy}, Loss: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
