{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dir = \"..\\isec-ic\\\\dataset\\\\train\"\n",
    "validation_dir = \"..\\isec-ic\\\\dataset\\\\valid\"\n",
    "test_dir = \"..\\isec-ic\\\\dataset\\\\test\"\n",
    "\n",
    "current_dir = os.path.dirname(os.path.realpath(__file__ if '__file__' in locals() else os.getcwd()))\n",
    "train_dir = os.path.join(current_dir, train_dir)\n",
    "validation_dir = os.path.join(current_dir, validation_dir)\n",
    "test_dir = os.path.join(current_dir, test_dir)\n",
    "\n",
    "categories = [\"bacterialspot\", \"healthy\",\"lateblight\",\"leafmold\", \"mosaicvirus\",\"yellowleafcurlvirus\" ,\"spidermite\",\"septorialeafspot\" ]\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    labels = []\n",
    "    subfolders = os.listdir(folder)\n",
    "    for subfolder in subfolders:\n",
    "        category = subfolder.split(\"_\")[-1].lower()\n",
    "        if category in categories:\n",
    "            print(\"Loading images from category: \" + category + \" and subfolder: \" + subfolder)\n",
    "            subfolder_path = os.path.join(folder, subfolder)\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                img = Image.open(os.path.join(subfolder_path, filename))\n",
    "                img = img.resize((64, 64))\n",
    "                img = np.array(img) / 255.0\n",
    "                images.append(img)\n",
    "                labels.append(categories.index(category))\n",
    "        else:\n",
    "            print(\"Category: \" + category + \" from subfolder \" + subfolder + \" is not in the list of categories.\")\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "x_train, y_train = load_images_from_folder(train_dir)\n",
    "x_validation, y_validation = load_images_from_folder(validation_dir)\n",
    "x_test, y_test = load_images_from_folder(test_dir)\n",
    "\n",
    "print(\"Images loaded successfully\")\n",
    "\n",
    "reduction_ratio = 0.8\n",
    "\n",
    "x_validation, x_reduction_validation, y_validation, y_reduction_validation = train_test_split(x_validation, y_validation, test_size=reduction_ratio, random_state=42)\n",
    "print(\"Conjunto de validação reduzido em 80%\")\n",
    "\n",
    "x_train, x_reduction, y_train, y_reduction = train_test_split(x_train, y_train, test_size=reduction_ratio, random_state=42)\n",
    "print(\"Conjunto de treino reduzido em 80%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
    "\n",
    "def create_cnn_model(dropout_rate, learning_rate):\n",
    "    #base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "    #base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(64, 64, 3)))\n",
    "    base_model = Xception(weights='imagenet', include_top=False, input_tensor=Input(shape=(64, 64, 3)))\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Flatten()(x)  # Flatten the output layer to 1 dimension\n",
    "    x = Dense(512, activation='relu')(x)   #  First dense layer\n",
    "    x = Dropout(dropout_rate)(x)  # Dropout to reduce overfitting\n",
    "    predictions = Dense(len(categories), activation='softmax')(x)  # Output layer\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Create_cnn_model function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "\n",
    "# Define a função para limitar a afinidade do processador\n",
    "def limitar_afinidade(processo, num_cores):\n",
    "    if num_cores < 1:\n",
    "        num_cores = 1\n",
    "    elif num_cores > psutil.cpu_count(logical=False):\n",
    "        num_cores = psutil.cpu_count(logical=False)\n",
    "\n",
    "    processo.cpu_affinity(list(range(num_cores)))\n",
    "\n",
    "# Obtém o ID do processo atual\n",
    "pid = os.getpid()\n",
    "\n",
    "# Define o número de núcleos desejado (metade dos núcleos disponíveis)\n",
    "metade_dos_cores = psutil.cpu_count(logical=False) // 2\n",
    "\n",
    "# Limita a afinidade do processador\n",
    "limitar_afinidade(psutil.Process(pid), metade_dos_cores)\n",
    "\n",
    "# Restaura a afinidade do processador para todos os núcleos após a execução do código\n",
    "limitar_afinidade(psutil.Process(pid), psutil.cpu_count(logical=False))\n",
    "\n",
    "\n",
    "if tf.test.is_gpu_available():\n",
    "    print(\"GPU disponível. Usando GPU para treinamento.\")\n",
    "else:\n",
    "    print(\"GPU não disponível. Usando CPU para treinamento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#dropout_rates = np.linspace(0.1, 1.0, 5)  # 5 valores igualmente espaçados entre 0.1 e 1.0\n",
    "#learning_rates = np.logspace(-4, -2, 5)   # 5 valores em escala logarítmica entre 0.0001 e 0.01\n",
    "\n",
    "# Especificando os valores diretamente\n",
    "#dropout_rates = [0.1, 0.325, 0.55, 0.775, 1.0]  # Valores específicos para dropout_rates\n",
    "#learning_rates = [0.0001, 0.00031623, 0.001, 0.00316228, 0.01]  # Valores específicos para learning_rates\n",
    "\n",
    "dropout_rates = [0.1, 0.325]  # Valores específicos para dropout_rates\n",
    "learning_rates = [0.0001]  # Valores específicos para learning_rates\n",
    "k_n_splits=2\n",
    "\n",
    "def evaluate_model_cv(dropout_rate, learning_rate, x_train, y_train, n_splits=5):\n",
    "    print(f\" - Evaluating with cross validation, splits={n_splits}\")\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    auc_scores = []\n",
    "\n",
    "    for train_index, val_index in kfold.split(x_train):\n",
    "        model = create_cnn_model(dropout_rate, learning_rate)\n",
    "        model.fit(x_train[train_index], y_train[train_index], epochs=5, batch_size=32, verbose=1)\n",
    "        \n",
    "        y_pred = model.predict(x_train[val_index])\n",
    "        y_val_categorical = to_categorical(y_train[val_index], num_classes=len(categories))\n",
    "        auc_score = roc_auc_score(y_val_categorical, y_pred, multi_class='ovr')\n",
    "        auc_scores.append(auc_score)\n",
    "\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "\n",
    "def grid_search(x_train, y_train, dropout_rates, learning_rates, n_splits=5):\n",
    "    best_score = -np.inf\n",
    "    best_params = {'dropout_rate': None, 'learning_rate': None}\n",
    "    best_auc_score = None\n",
    "\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for learning_rate in learning_rates:\n",
    "            print(f\"Testing params: dropout_rate: {dropout_rate}, learning_rate: {learning_rate}\")\n",
    "            auc_score = evaluate_model_cv(dropout_rate, learning_rate, x_train, y_train, n_splits)\n",
    "\n",
    "            if auc_score > best_score:\n",
    "                best_score = auc_score\n",
    "                best_params = {'dropout_rate': dropout_rate, 'learning_rate': learning_rate}\n",
    "                best_auc_score = auc_score\n",
    "                print(f\"New best score: AUC={auc_score}, with parameters: dropout_rate={dropout_rate}, learning_rate={learning_rate}\")\n",
    "\n",
    "    return best_params, best_auc_score\n",
    "\n",
    "\n",
    "best_params, best_auc_score = grid_search(x_train, y_train, dropout_rates, learning_rates, n_splits=k_n_splits)\n",
    "print(f\"Melhores parâmetros encontrados: {best_params}, Melhor pontuação AUC: {best_auc_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Avaliando o modelo no conjunto de teste...\")\n",
    "loss_test, test_accuracy = best_model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"Acurácia no Teste: {test_accuracy}, Loss: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
